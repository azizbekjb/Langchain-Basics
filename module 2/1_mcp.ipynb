{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3807e4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5dc3fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c61e3e3",
   "metadata": {},
   "source": [
    "# Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c166bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "            \"transport\": 'stdio',\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"resources/1_mcp_server.py\"]\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736f199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources \n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts \n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a079219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.agents import create_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "524bd214",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(model=\"gemini-2.5-flash\", model_provider=\"google_genai\")\n",
    "agent = create_agent(\n",
    "    model = model,\n",
    "    tools=tools,\n",
    "    system_prompt = prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a2a12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e49483df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='5fe0be9f-1f74-4b23-b504-3f2aedaf3cb0'),\n",
      "              AIMessage(content='', additional_kwargs={'function_call': {'name': 'search_web', 'arguments': '{\"query\": \"langchain-mcp-adapters library\"}'}, '__gemini_function_call_thought_signatures__': {'ccb9c664-02b0-4eeb-b8f0-395ce2c66499': 'CtEBAXLI2nzuChb41cS7PcYDWxKB1LPAXf6LdEez3ApRBniSotwH1o+2GF932oEuIF4n2OvEOG4NT9FDhuwPS6l5jTlAdqCQsgAuXgfqJiBwbTKiCYksfyGLuumiKNKQxTUhbtXZDbcRHaifj/+jngCEx7yWrNP8W3chLMZp+4SH0w3EseZCB5DdHzjSAVlUOmzR2WPg4h10NqlDhIceMKFqTaqk/4Y4GquLevwn1z3Y60x3j80IQEUybKZwQ7fB4LMlAY2DRVvES8dJn22tLys1wxU='}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019bc7c3-bd4b-7b81-8ba5-81e1a4de11ab-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters library'}, 'id': 'ccb9c664-02b0-4eeb-b8f0-395ce2c66499', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 195, 'output_tokens': 64, 'total_tokens': 259, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 42}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters library\",\\n  \"response_time\": 0.68,\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://pypi.org/project/langchain-mcp-adapters/\",\\n      \"title\": \"langchain-mcp-adapters - PyPI\",\\n      \"content\": \"This library provides a lightweight wrapper that makes Anthropic Model Context Protocol (MCP) tools compatible with LangChain and LangGraph.\",\\n      \"score\": 0.99999285,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.9999875,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchainjs-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchainjs-mcp-adapters: ** THIS REPO ... - GitHub\",\\n      \"content\": \"This library provides a lightweight wrapper to allow Model Context Protocol (MCP) services to be used with LangChain.js.\",\\n      \"score\": 0.9999831,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://reference.langchain.com/python/langchain_mcp_adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"# `langchain-mcp-adapters`¶. Client for connecting to multiple MCP servers and loading LC tools/resources. This module provides the `MultiServerMCPClient` class for managing connections to multiple MCP servers and loading tools, prompts, and resources from them. Loads LangChain-compatible tools, prompts and resources from MCP servers. | \\\\\\\\_\\\\\\\\_init\\\\\\\\_\\\\\\\\_ (`langchain_mcp_adapters.client.MultiServerMCPClient.__init__`)\\\\\">\\\\\\\\_\\\\\\\\_init\\\\\\\\_\\\\\\\\_ | Initialize a `MultiServerMCPClient` with MCP servers connections. | session  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.session`)\\\\\">session | Connect to an MCP server and initialize a session. | get\\\\\\\\_tools  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_tools`)\\\\\">get\\\\\\\\_tools | Get a list of all tools from all connected servers. | get\\\\\\\\_resources  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_resources`)\\\\\">get\\\\\\\\_resources | Get resources from MCP server(s). **TYPE:** `dict[str,`  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\\\\\">Connection] | None   **DEFAULT:** `None` |. ### load\\\\\\\\_mcp\\\\\\\\_tools `async` ¶. (langchain_mcp_adapters.callbacks.Callbacks)\\\\\">Callbacks | None = None,  tool_interceptors: list[            ToolCallInterceptor (langchain_mcp_adapters.interceptors.ToolCallInterceptor)\\\\\">ToolCallInterceptor] | None = None,  server_name: str | None = None,  tool_name_prefix: bool = False, ) -> list[            BaseTool (langchain_core.tools.BaseTool)\\\\\">BaseTool]. **TYPE:**  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\\\\\">Connection | None   **DEFAULT:** `None` |. ### load\\\\\\\\_mcp\\\\\\\\_resources `async` ¶. | \\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_  `async`  (`langchain_mcp_adapters.interceptors.ToolCallInterceptor.__call__`)\\\\\">\\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_ | Intercept tool execution with control over handler invocation.\",\\n      \"score\": 0.9999813,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.npmjs.com/package/@langchain/mcp-adapters\",\\n      \"title\": \"langchain/mcp-adapters - NPM\",\\n      \"content\": \"The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \\\\\"working-server\\\\\".\",\\n      \"score\": 0.99993694,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"request_id\": \"83568b30-31f5-4e64-8ed6-b04340763dd1\"\\n}', 'id': 'lc_4ca85222-5a38-4ac6-b51f-68343e0f9a63'}], name='search_web', id='83373089-1765-479d-b35e-c46df5af5a61', tool_call_id='ccb9c664-02b0-4eeb-b8f0-395ce2c66499', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters library', 'response_time': 0.68, 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://pypi.org/project/langchain-mcp-adapters/', 'title': 'langchain-mcp-adapters - PyPI', 'content': 'This library provides a lightweight wrapper that makes Anthropic Model Context Protocol (MCP) tools compatible with LangChain and LangGraph.', 'score': 0.99999285, 'raw_content': None}, {'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.9999875, 'raw_content': None}, {'url': 'https://github.com/langchain-ai/langchainjs-mcp-adapters', 'title': 'langchain-ai/langchainjs-mcp-adapters: ** THIS REPO ... - GitHub', 'content': 'This library provides a lightweight wrapper to allow Model Context Protocol (MCP) services to be used with LangChain.js.', 'score': 0.9999831, 'raw_content': None}, {'url': 'https://reference.langchain.com/python/langchain_mcp_adapters/', 'title': 'langchain-mcp-adapters', 'content': '# `langchain-mcp-adapters`¶. Client for connecting to multiple MCP servers and loading LC tools/resources. This module provides the `MultiServerMCPClient` class for managing connections to multiple MCP servers and loading tools, prompts, and resources from them. Loads LangChain-compatible tools, prompts and resources from MCP servers. | \\\\_\\\\_init\\\\_\\\\_ (`langchain_mcp_adapters.client.MultiServerMCPClient.__init__`)\">\\\\_\\\\_init\\\\_\\\\_ | Initialize a `MultiServerMCPClient` with MCP servers connections. | session  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.session`)\">session | Connect to an MCP server and initialize a session. | get\\\\_tools  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_tools`)\">get\\\\_tools | Get a list of all tools from all connected servers. | get\\\\_resources  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_resources`)\">get\\\\_resources | Get resources from MCP server(s). **TYPE:** `dict[str,`  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\">Connection] | None   **DEFAULT:** `None` |. ### load\\\\_mcp\\\\_tools `async` ¶. (langchain_mcp_adapters.callbacks.Callbacks)\">Callbacks | None = None,  tool_interceptors: list[            ToolCallInterceptor (langchain_mcp_adapters.interceptors.ToolCallInterceptor)\">ToolCallInterceptor] | None = None,  server_name: str | None = None,  tool_name_prefix: bool = False, ) -> list[            BaseTool (langchain_core.tools.BaseTool)\">BaseTool]. **TYPE:**  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\">Connection | None   **DEFAULT:** `None` |. ### load\\\\_mcp\\\\_resources `async` ¶. | \\\\_\\\\_call\\\\_\\\\_  `async`  (`langchain_mcp_adapters.interceptors.ToolCallInterceptor.__call__`)\">\\\\_\\\\_call\\\\_\\\\_ | Intercept tool execution with control over handler invocation.', 'score': 0.9999813, 'raw_content': None}, {'url': 'https://www.npmjs.com/package/@langchain/mcp-adapters', 'title': 'langchain/mcp-adapters - NPM', 'content': 'The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \"working-server\".', 'score': 0.99993694, 'raw_content': None}], 'request_id': '83568b30-31f5-4e64-8ed6-b04340763dd1'}}}),\n",
      "              AIMessage(content=[{'type': 'text', 'text': 'The `langchain-mcp-adapters` library provides a lightweight wrapper that makes Anthropic Model Context Protocol (MCP) tools compatible with LangChain and LangGraph.\\n\\nIts key functionalities include:\\n*   Converting MCP tools into tools that are compatible with both LangChain and LangGraph.\\n*   Enabling interaction with tools hosted on multiple MCP servers.\\n*   Allowing for the seamless integration of existing MCP tool servers into LangGraph Agents.\\n\\nThe library aims to simplify the process of connecting LangChain and LangGraph with the growing ecosystem of MCP tool servers, eliminating the need for manual adaptation of each tool. It also allows agents to utilize tools from various MCP servers simultaneously, facilitating the creation of more powerful applications. The library includes a `MultiServerMCPClient` class to manage connections to these servers and load tools, prompts, and resources. There are both Python and JavaScript versions of this library.', 'extras': {'signature': 'Cq4JAXLI2nz8QOzqu6LOu8SXjtlKRlc2HCEukTx66A1q9m9AqTeC32dNouycP5AYuWs70IflZB+Qak1qQFOb4oz9XdM+5wmZBMxDeKXju+mTarJa+nB7tW9DJcr3HTTJimYN4BwRnmEJ0scQh3fxxZVZdZuzsS5wBMO1iyB9wwADXfs8IoFc4LdL0VwneGs3q3ezqW3iRFnMuFCoSJ45oVIXjIbvhI+CdT3TzLxVFRaLBSOPEhFOS2Ll8GfDPVjWuyi4E25TV7PxlUdIZSkpSjc9auwPzHPbUbOCaWnrnDYxkwcvG1XcqV6nPeqTzqWKtLfnybLs1Metw0slQ3IvmhJbkYZHfcJoCD1CaUjmGO27dV/BcLad5ErVXRg8n568rj8y6wGQJ3UfyNNacYWjGjZ4W+V18EkcoWabtJQvkSuVaFHnYJBEdIiYiuMeC4LF5NZuwTTF6P6s6KHMOlnCLvIw3DlCF4/RFG07MjdjPraYfXRcR9ZBSH4eLZB6yOsGHwQhekf07QCTTpN+FDTyAvfLHhiPQx4VP18yM/z3943ZbbDwyl/ktKPXqPB0NhDNuDsyYVniDsx+2TssW9zd/8J+7yVggEtr8j4joCrN3YJG6H71PXcWeqvtRwsZAlSBEr2HltQdrntu5jC3ZSkjhrgGEFelmLQ4etBPCgoBueGz+zkuu95ifkQLeFlT3e/kOso2IuA/kF/3ahPhlqnPER9sFp1gKA+pQUnh0UbRzXwySD9CisV5/zH+JFIkVI4fUA1ivutxkm9iynV6rFMZEkNl4nVWE3HkCjVq+dEE2fgy8YiOMxG08E/XdSGp/0P6QNTIsdC73Ga7O+qgr2UcpVk03brqR+R/PDY4EJdV0haZ4k6WFB1VCjWRKyMo1FQCxtNDUgATVNKrvDdrM7WA6KWpJx6BC2YTKEt0TAA/c7RsHAGDG37D3FhogIQAUezgofbFdwok7FO4fp28IWIPkE9gQsRwpHD/VtR8fw08sFBre+8phKERyPdRiWUpbsnk6SzggnzMorZH6y8yGzUFpnto3DkVh7D0N9yNxm7xspnwtsHeWHMv0oucCC7bJGdVx58rhNA4wdfbiVRB1tZDKMOe2uexDBSnby591OfIbrKEoLkAcajQgeanjr04jgTSEpXVPttpx+v+WGH6vLm1W56A/ujhKBWwpItDJspz+x9ADIN23ItruRZgkyx6meaWxgfKUctAgeQ0KpXDxEPDoeFBHZRAvfOfGZbBoZCP1b25NTKW0++z65XVjkApxQO2/FsItkiORSNTX8NmzRkFDAez/JGCH43LbRmjKxOF/9Az6NGClke+VQgV8HcOrXNKaNhnoJWrl+5btSfZNJR2ewhWkrVRBil8vKlCZnQc9TdybLfGI/36i5u3KEPRMFpHwpEFtlVg+ejdZkcZG882rQuN9BTqlpyN424v3hSG6T5z8rFl/K2HV+DY423nCmY7je5nplOYW7i9V+9ZKXA68gqDnPaaWx05GB0/hnecU/qzkzrVmc3vqARiyakv9FY9AualyJGGSOtyijo958o5lfglsx4G0lBWIZ1JGLzK2sGXMwaZ6SB0H59qT6xebaMCdg=='}}], additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019bc7c3-ea11-7c11-84ac-ebea3ae00b4d-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 1825, 'output_tokens': 437, 'total_tokens': 2262, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 253}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff3f5e",
   "metadata": {},
   "source": [
    "# Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c47be35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d886c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4222cce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='4d9650fe-8cc2-41a0-bd68-8a8b982dfdf9'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'get_current_time', 'arguments': '{\"timezone\": \"America/New_York\"}'}, '__gemini_function_call_thought_signatures__': {'7ac4ce10-8876-47ff-bb70-25fc4ba85be0': 'Cu4CAXLI2nwKuHf1/ZeV/4TV+ASbOSr85WmHdO0L3KJoLvL8hn1mfUE4SB6dLhZUTidebZv/6vjbDgOZefyn0mlSDPpLLqBoqIvcRixLtkobL7pELXw4pZM6iFR9JiXpONxjJLUvStAjIOMfmJ7UT59KL2RsBdZ3pl21ODS4rxuu2ioCGPSNLYsROnqvcK58js5XHTvRf3Try0iYt2qllJ4i94J0FfnyXTurnTJDUlmpcLIe/KeK39XmkG2GpEdVLAst6gvHuJ1JxVY7VG6t9tPa1IkJBlq2tdadzCuYGYDPh20nwxVeG935nUPBYCFxkNbTej9unNqU2SFRM/nHmO4ozeDN6jD7WRBy27CN8MsbaYEiufLhOSdv1JyQQnR2m1jkpbNrOFFhZTHgGKIlbKcHGvDiHjO+UG3v+jdAQou7LdVAEkJsQO3WExoUV1iqSjNSLSoCsz7dFDpcNZ07w+L860mGsEqAUP/W3yk2s8Ve'}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019bc7c4-085e-7bc2-9a4f-6b3cef1471da-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'America/New_York'}, 'id': '7ac4ce10-8876-47ff-bb70-25fc4ba85be0', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 250, 'output_tokens': 102, 'total_tokens': 352, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 81}}), ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"America/New_York\",\\n  \"datetime\": \"2026-01-16T12:04:37-05:00\",\\n  \"day_of_week\": \"Friday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_d01f5c9a-a3d5-461d-80ac-1fcba698ee9b'}], name='get_current_time', id='f30f08be-9011-472a-8f01-feab3525586b', tool_call_id='7ac4ce10-8876-47ff-bb70-25fc4ba85be0'), AIMessage(content='The current time in America/New_York is 12:04:37 on Friday, January 16, 2026.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019bc7c4-1df1-7011-8e16-3c4485ffe022-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 407, 'output_tokens': 33, 'total_tokens': 440, 'input_token_details': {'cache_read': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f545a0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current time in America/New_York is 12:04:37 on Friday, January 16, 2026.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"messages\"][-1].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
