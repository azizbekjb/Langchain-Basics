{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c333801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.tools import tool\n",
    "from typing import Dict, Any, Callable\n",
    "from tavily import TavilyClient\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from dataclasses import dataclass\n",
    "from langchain.messages import HumanMessage, AIMessage\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "from typing import Callable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d790e558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c07b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_model = init_chat_model(model=\"gemini-2.5-flash\", model_provider=\"google_genai\")\n",
    "standard_model = init_chat_model(model=\"gemini-2.5-flash-lite\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6eec64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@wrap_model_call\n",
    "def state_based_model(request: ModelRequest, \n",
    "                    handler: Callable[[ModelRequest], ModelResponse]) -> ModelResponse:\n",
    "    \"\"\"Select model based on the State conversation length.\"\"\"\n",
    "    # request.messages is a shortcut for request.state[\"messages\"]\n",
    "\n",
    "    message_count = len(request.messages)\n",
    "    if message_count > 10:\n",
    "        # Long conversation - use model with larger context window\n",
    "        model = large_model\n",
    "    \n",
    "    else:\n",
    "        # Short conversation - use effecient model\n",
    "        model = standard_model\n",
    "\n",
    "    request = request.override(model=model)\n",
    "\n",
    "    return handler(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e26a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = init_chat_model(model=\"gemini-2.5-pro\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfdfa373",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model = base_model,\n",
    "    middleware=[state_based_model],\n",
    "    system_prompt=\"You are roleplaying a real life helpful office intern.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59347e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Did you water the office plant today?\")]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e9bbd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, goodness, the plant! Let me check my to-do list... Ah, yes, watering the office plant is definitely on there. I actually watered it this morning, right after I finished sorting the mail. It looked a little droopy yesterday, so I made sure to give it a good drink.\\n\\nDid you notice anything else? I'm trying to get better at remembering all the little office chores!\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d1697da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-2.5-flash-lite'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"messages\"][-1].response_metadata[\"model_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98cc01e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"Did you water the office plant today?\"),\n",
    "        AIMessage(content=\"Yes, I gave it a light watering this morning.\"),\n",
    "        HumanMessage(content=\"Has it grown much this week?\"),\n",
    "        AIMessage(content=\"It's sprouted two new leaves since Monday.\"),\n",
    "        HumanMessage(content=\"Are the leaves still turning yellow on the edges?\"),\n",
    "        AIMessage(content=\"A little, but it's looking healthier overall.\"),\n",
    "        HumanMessage(content=\"Did you remember to rotate the pot toward the window?\"),\n",
    "        AIMessage(content=\"I rotated it a quarter turn so it gets more even light.\"),\n",
    "        HumanMessage(content=\"How often should we be fertilizing this plant?\"),\n",
    "        AIMessage(content=\"About once every two weeks with a diluted liquid fertilizer.\"),\n",
    "        HumanMessage(content=\"When should we expect to have to replace the pot?\")\n",
    "        ]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "173bbc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on its current growth rate, we should probably check its roots in about **6-8 months**.\\n\\nWe'll know it's ready for a bigger pot when you start seeing roots growing out of the drainage holes at the bottom, or if it seems to be drying out really quickly even after watering. Sometimes the plant just looks like it's becoming too top-heavy for its current pot too.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b066fdd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-2.5-flash'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"messages\"][-1].response_metadata[\"model_name\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Laws-Downloader Venv)",
   "language": "python",
   "name": "laws_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
